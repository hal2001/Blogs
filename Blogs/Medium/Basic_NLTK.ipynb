{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "\n",
    "testString = 'First, we must be good stewards of this economy, and renew the great institutions on which millions of our fellow citizens rely. \\\n",
    "\t\t\t   America\\'s economy is the fastest growing of any major industrialized nation. In the past four years, \\\n",
    "\t\t\t   we provided tax relief to every person who pays income taxes, overcome a recession, opened up new markets abroad,\\\n",
    "\t\t\t   prosecuted corporate criminals, raised homeownership to its highest level in history, and in the last year alone, \\\n",
    "\t\t\t   the United States has added 2.3 million new jobs. (Applause.) When action was needed, the Congress delivered -- and the nation is grateful.'\n",
    "\n",
    "\n",
    "\n",
    "def tokenizeIntoSentencesAndWords(testString,doPrint):\n",
    "\ttokenized_sentences = sent_tokenize(testString)\n",
    "\ttokenized_words = word_tokenize(testString)\n",
    "\n",
    "\tif doPrint is True:\n",
    "\t\tprint(\"Tokenized Words:\\n\\n\",tokenized_words,\"\\n\\n\")\n",
    "\n",
    "\t\tprint(\"Tokenized Sentences:\\n\\n\",tokenized_sentences,\"\\n\\n\")\n",
    "\n",
    "\t\tfor sentences in tokenized_sentences:\n",
    "\t\t\tprint(sentences,\"\\n\")\n",
    "\n",
    "\treturn tokenized_words , tokenized_sentences\n",
    "\n",
    "\n",
    "def removeStopWords(word_tokens,doPrint):\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\n",
    "\tfiltered_words = []\n",
    "\n",
    "\tfor w in word_tokens:\n",
    "\t\tif not w in stop_words:\n",
    "\t\t\tfiltered_words.append(w)\t\n",
    "\n",
    "\tif doPrint is True:\n",
    "\t\tprint(\"\\n\\nWords after Removing Stop words:\\n\")\n",
    "\t\t\n",
    "\t\tprint(filtered_words)\n",
    "\n",
    "\treturn filtered_words\n",
    "\n",
    "def lemmatizeWords(wordsToLemmatize,doPrint):\n",
    "\tlemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\tlemmatizedWords  = []\n",
    "\n",
    "\tfor words in wordsToLemmatize:\n",
    "\t\tlemmatizedWords.append(lemmatizer.lemmatize(words))\n",
    "\n",
    "\tif doPrint is True:\n",
    "\t\tprint(lemmatizedWords)\n",
    "\t\n",
    "\treturn lemmatizedWords\n",
    "\n",
    "\n",
    "def POSTagging(wordsToTag,doPrint,seeDocs):\n",
    "\n",
    "\tPOSTaggedWords = pos_tag(wordsToTag)\n",
    "\n",
    "\tif doPrint is True:\n",
    "\t\tprint(POSTaggedWords)\n",
    "\n",
    "\tif seeDocs is True:\n",
    "\t\tprint(nltk.help.upenn_tagset())\n",
    "\n",
    "\treturn POSTaggedWords\n",
    "\n",
    "\n",
    "\n",
    "word_tokens , sentence_tokens = tokenizeIntoSentencesAndWords(testString,doPrint=False)\n",
    "filtered_words = removeStopWords(word_tokens,doPrint=False)\n",
    "lemmatizedWords = lemmatizeWords(filtered_words,doPrint=False)\n",
    "POSTaggedWords = POSTagging(lemmatizedWords,doPrint=False,seeDocs=False)\n",
    "\n",
    "# print(type(['this','is','fucking','awful']))\n",
    "# print(pos_tag(lemmatizedWords))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
